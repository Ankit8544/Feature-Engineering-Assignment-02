{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q.No-01    What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans :-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Min-Max scaling** is also known as **Min-Max normalization**. It is a common data preprocessing technique used in machine learning and statistics to transform numerical features within a specific range. The goal of Min-Max scaling is to rescale the data so that it falls within a specified range, typically between 0 and 1, or any other user-defined range. This helps to ensure that all features have a similar scale, which can be important for various machine learning algorithms that are sensitive to the scale of input features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The Min-Max scaling formula for a single feature is as follows :-**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$X_{\\text{scaled}} = \\frac{X - X_{\\text{min}}}{X_{\\text{max}} - X_{\\text{min}}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Where :-**\n",
    "\n",
    "- $X_{\\text{scaled}}$ is the scaled value of the feature $X$.\n",
    "\n",
    "- $X$ is the original value of the feature.\n",
    "\n",
    "- $X_{\\text{min}}$ is the minimum value of the feature in the dataset.\n",
    "\n",
    "- $X_{\\text{max}}$ is the maximum value of the feature in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The resulting $X_{\\text{scaled}}$ will always be within the range [0, 1].**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`Here's an example to illustrate Min-Max scaling in Math` :-**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   -   **Suppose you have a dataset of ages for a group of individuals :**\n",
    "        \n",
    "       **Age - 25 ,30 ,35 ,40 ,45**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   -   **To perform Min-Max scaling on this dataset, you need to calculate the minimum and maximum values of the \"Age\"   feature :**\n",
    "\n",
    "         - Minimum age ($X_{\\text{min}}$) = 25\n",
    "         \n",
    "         - Maximum age ($X_{\\text{max}}$) = 45"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   -   **Now, you can apply the Min-Max scaling formula to each age value :**\n",
    "\n",
    "         - For \\(X = 25\\) : $X_{\\text{scaled}} = \\frac{25 - 25}{45 - 25} = 0$\n",
    "\n",
    "         - For \\(X = 30\\) : $X_{\\text{scaled}} = \\frac{30 - 25}{45 - 25} = 0.25$\n",
    "\n",
    "         - For \\(X = 35\\) : $X_{\\text{scaled}} = \\frac{35 - 25}{45 - 25} = 0.5$\n",
    "\n",
    "         - For \\(X = 40\\) : $X_{\\text{scaled}} = \\frac{40 - 25}{45 - 25} = 0.75$\n",
    "\n",
    "         - For \\(X = 45\\) : $X_{\\text{scaled}} = \\frac{45 - 25}{45 - 25} = 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   -   **After applying Min-Max scaling, your dataset of ages will be transformed as follows :**\n",
    "\n",
    "$$Age (Scaled)\\ -$$\n",
    "$$0.00$$\n",
    "$$0.25$$\n",
    "$$0.50$$\n",
    "$$0.75$$\n",
    "$$1.00$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`Here's an example to illustrate Min-Max scaling in Python` :-**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ages_Data (original)</th>\n",
       "      <th>Ages_Data (Min-Max scaled)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>35</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>40</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>45</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Ages_Data (original)  Ages_Data (Min-Max scaled)\n",
       "0                    25                        0.00\n",
       "1                    30                        0.25\n",
       "2                    35                        0.50\n",
       "3                    40                        0.75\n",
       "4                    45                        1.00"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "\n",
    "# Original dataset of ages\n",
    "Ages_Data = [25, 30, 35, 40, 45]\n",
    "\n",
    "Q_No_01_df = pd.DataFrame({\"Ages_Data (original)\" : Ages_Data})\n",
    "\n",
    "min_max=MinMaxScaler()\n",
    "\n",
    "min_max.fit(Q_No_01_df[['Ages_Data (original)']])\n",
    "\n",
    "Q_No_01_df['Ages_Data (Min-Max scaled)'] = pd.DataFrame(min_max.transform(Q_No_01_df[['Ages_Data (original)']]))\n",
    "\n",
    "display(Q_No_01_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Now, all the age values are within the range [0, 1], which can be useful for various machine learning algorithms that require features to have a consistent scale. Min-Max scaling helps prevent features with larger numeric values from dominating the learning process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q.No-02    What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling? Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans :-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The Unit Vector technique** in feature scaling is a method used to scale numerical features in a dataset so that they have a magnitude (length) of 1 while preserving the direction of the data. This scaling technique is also known as \"Normalization\" or \"L2 Normalization.\" It is particularly useful when you want to ensure that all features contribute equally to machine learning algorithms, especially those that rely on distances or magnitudes of feature vectors, such as clustering algorithms or support vector machines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main idea behind Unit Vector scaling is to transform each feature in a way that makes it lie on the unit circle or unit sphere (in 2D or 3D, respectively). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`Here's how it works for a single feature` :-**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Calculate the magnitude (length) of the feature vector :**\n",
    "\n",
    "   - For a 2D vector (a single feature), it's the square root of the sum of the squares of its values.\n",
    "\n",
    "   - For a 3D vector (a single feature), it's the square root of the sum of the squares of its values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Divide each value of the feature by its magnitude, effectively scaling it down so that the length of the feature vector becomes 1.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This scaling technique ensures that the direction of the feature vector is preserved, but the magnitude is set to 1. It is different from Min-Max scaling in that Min-Max scaling transforms features to a specific range (usually between 0 and 1) without considering the direction of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`Here's an example to illustrate the Unit Vector scaling technique in Python` :-**\n",
    "\n",
    "   -   Suppose you have a dataset with two features, **\"Height\"** and **\"Weight\"** and you want to normalize them using Unit Vector scaling. Here's what you would do:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Height (Cm)</th>\n",
       "      <th>Normalized Height</th>\n",
       "      <th>Weight (Kg)</th>\n",
       "      <th>Normalized Weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>150</td>\n",
       "      <td>0.948683</td>\n",
       "      <td>50</td>\n",
       "      <td>0.316228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>160</td>\n",
       "      <td>0.936329</td>\n",
       "      <td>60</td>\n",
       "      <td>0.351123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>170</td>\n",
       "      <td>0.924678</td>\n",
       "      <td>70</td>\n",
       "      <td>0.380750</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Height (Cm)  Normalized Height  Weight (Kg)  Normalized Weight\n",
       "0          150           0.948683           50           0.316228\n",
       "1          160           0.936329           60           0.351123\n",
       "2          170           0.924678           70           0.380750"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "# Create a sample dataset\n",
    "data = {'Height': [150, 160, 170],\n",
    "        'Weight': [50, 60, 70]}\n",
    "Q_No_02_df1 = pd.DataFrame(data)\n",
    "Height = Q_No_02_df1['Height'].to_list()\n",
    "Weight = Q_No_02_df1['Weight'].to_list()\n",
    "\n",
    "# Create a new DataFrame with the normalized data\n",
    "Q_No_02_df2 = pd.DataFrame(normalize(Q_No_02_df1[['Height', 'Weight']]),columns=['Normalized Height' , 'Normalized Weight'])\n",
    "Normalized_Height = Q_No_02_df2['Normalized Height'].to_list()\n",
    "Normalized_Weight = Q_No_02_df2['Normalized Weight'].to_list()\n",
    "\n",
    "Q_No_02_Final_df = pd.DataFrame({\n",
    "    'Height (Cm)' : Height ,\n",
    "    'Normalized Height': Normalized_Height,\n",
    "    'Weight (Kg)': Weight,\n",
    "    'Normalized Weight': Normalized_Weight\n",
    "})\n",
    "\n",
    "display(Q_No_02_Final_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q.No-03    What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans :-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`Principal Component Analysis` (PCA)** is a dimensionality reduction technique widely used in data analysis and machine learning. It aims to reduce the dimensionality of a dataset while preserving as much of the original variability or information as possible. PCA achieves this by transforming the data into a new coordinate system, where the new axes, called principal components, are linear combinations of the original features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`Here's a step-by-step explanation of how PCA works and an example to illustrate its application` :-**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1: Standardize the data**\n",
    "\n",
    "- The first step in PCA is to standardize the dataset. This involves centering the data (subtracting the mean) and scaling it (dividing by the standard deviation) to ensure that all features have the same scale. Standardizing the data is essential to give equal weight to all features during the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2: Calculate the covariance matrix**\n",
    "\n",
    "- After standardization, you calculate the covariance matrix of the data. The covariance matrix describes how each feature varies with respect to every other feature. It helps identify relationships and dependencies between features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3: Compute the eigenvectors and eigenvalues of the covariance matrix**\n",
    "\n",
    "- The next step involves finding the eigenvectors and eigenvalues of the covariance matrix. Eigenvectors represent the directions in which the data varies the most, while eigenvalues indicate the amount of variance explained by each eigenvector. The eigenvectors are also referred to as the principal components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 4: Select the top k principal components**\n",
    "\n",
    "- You can decide to retain a subset of the top k principal components that explain the most variance in the data. Typically, you choose k such that it captures a sufficiently high percentage of the total variance (e.g., 95% of the variance)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 5: Project the data onto the new coordinate system**\n",
    "\n",
    "- Finally, you project the original data onto the new coordinate system defined by the selected principal components. This transformation reduces the dimensionality of the data while retaining most of the important information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`Example` :-**\n",
    "Let's say you have a dataset of houses with various features like square footage, number of bedrooms, number of bathrooms, and location. You want to reduce the dimensionality of the dataset for analysis or visualization purposes.\n",
    "\n",
    "1. **Standardize the data:** Calculate the mean and standard deviation for each feature and use them to standardize the data.\n",
    "\n",
    "2. **Calculate the covariance matrix:** Compute the covariance matrix to understand how these features are related and how they vary together.\n",
    "\n",
    "3. **Compute eigenvectors and eigenvalues:** Calculate the eigenvectors and eigenvalues of the covariance matrix. These will represent the principal components and their importance in explaining the data's variance.\n",
    "\n",
    "4. **Select the top k principal components:** You decide to keep the first two principal components as they explain 90% of the variance in the data.\n",
    "\n",
    "5. **Project the data:** Project the original data onto the new two-dimensional space defined by the first two principal components. Now, you have reduced the dimensionality from, say, four features (square footage, bedrooms, bathrooms, location) to just two principal components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PCA** is particularly useful for reducing the dimensionality of high-dimensional data while retaining essential information, simplifying visualization, and improving the efficiency of machine learning algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q.No-04    What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature Extraction? Provide an example to illustrate this concept."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans :-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`Principal Component Analysis` (PCA)** is a dimensionality reduction technique that can be used for feature extraction in machine learning and data analysis. It helps in simplifying the complexity of high-dimensional data by transforming it into a new set of orthogonal variables called principal components. These principal components are linear combinations of the original features and are ordered by the amount of variance they explain in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`PCA can be used for feature extraction in the following way` :-**\n",
    "\n",
    "1. **Data Transformation**: PCA transforms the original feature space into a new space defined by the principal components. The first principal component (PC1) explains the most variance in the data, the second (PC2) explains the second most, and so on. By selecting a subset of these principal components, you can effectively reduce the dimensionality of the data while preserving most of the variance.\n",
    "\n",
    "2. **Feature Selection**: Instead of using all the original features, you can choose to retain only a subset of the principal components. This selection is based on how much variance you want to retain in your reduced-dimensional data. By selecting a smaller number of principal components, you can reduce the dimensionality of the data while still capturing the most important patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`Here's an example to illustrate how PCA can be used for feature extraction` :-**\n",
    "\n",
    "-   Suppose you have a dataset with 1000 samples and 20 features. You want to reduce the dimensionality of this dataset for a machine learning task while preserving as much information as possible.\n",
    "\n",
    "      1. **Standardization**: Start by standardizing the data (mean centering and scaling) so that all features have zero mean and unit variance. This step is essential for PCA.\n",
    "\n",
    "      2. **PCA**: Apply PCA to the standardized data. As a result, you obtain 20 principal components, ranked in order of the amount of variance they explain.\n",
    "\n",
    "      3. **Variance Explained**: Calculate the cumulative explained variance as you add more principal components. You might find that the first five principal components explain 90% of the variance in the data.\n",
    "\n",
    "      4. **Feature Extraction**: Select a subset of the principal components based on the desired variance retention. In this case, you choose the first five principal components.\n",
    "\n",
    "      5. **Reduced-Dimensional Data**: Transform your original data using these five principal components. You now have a new dataset with only five features, which captures most of the important information from the original 20 features.\n",
    "\n",
    "      6. **Machine Learning**: You can now use this reduced-dimensional dataset for your machine learning task. Since you've retained the most important information, you may find that your model performs well with significantly fewer features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PCA** for feature extraction is particularly useful when you have high-dimensional data and want to reduce computational complexity or deal with multicollinearity among features. It helps to remove noise and redundancy while retaining the essential information for analysis or modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q.N0-05    PCA for feature extraction is particularly useful when you have high-dimensional data and want to reduce computational complexity or deal with multicollinearity among features. It helps to remove noise and redundancy while retaining the essential information for analysis or modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans :-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`PCA` (Principal Component Analysis)** is indeed a powerful technique for feature extraction and dimensionality reduction, and your statement summarizes its key benefits and purposes well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`Here's a more detailed explanation of why PCA is useful in these scenarios` :-**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **High-Dimensional Data Reduction:** When dealing with datasets containing a large number of features (high dimensionality), the computational complexity of many machine learning algorithms increases significantly. PCA allows you to reduce the number of dimensions while retaining most of the essential information, making subsequent analysis and modeling more efficient and less prone to overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Multicollinearity Mitigation:** Multicollinearity occurs when two or more features in your dataset are highly correlated, making it difficult for models to discern the individual effects of these features on the target variable. PCA helps to decorrelate the features by transforming them into a new set of orthogonal (uncorrelated) variables called principal components. This can improve the stability and interpretability of your models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Noise Reduction:** High-dimensional data often contains noise or irrelevant information. PCA identifies and focuses on the principal components that capture the most significant variability in the data. By discarding the less important components (which often represent noise), PCA can enhance the signal-to-noise ratio in your data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. **Dimensionality Reduction with Minimal Information Loss:** PCA selects the principal components in descending order of the variance they explain. This means that the first few principal components retain most of the variation in the original data. You can choose how many components to retain based on a desired level of explained variance. By retaining a relatively small number of components, you can achieve dimensionality reduction while preserving most of the crucial information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. **Visualization:** PCA can be used for visualizing high-dimensional data in two or three dimensions. By projecting the data onto a lower-dimensional subspace defined by the principal components, you can create scatterplots or other visualizations that help you understand the underlying structure of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. **Improved Model Performance:** When you reduce dimensionality with PCA, you often simplify the modeling process. Simpler models are less prone to overfitting and may generalize better to new data, leading to improved model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`However`, it's important to note that PCA also has some limitations and considerations, such as the loss of interpretability of the original features, potential loss of some information when reducing dimensions, and the need to standardize or normalize data before applying PCA. Additionally, PCA assumes linear relationships between variables, which may not always hold in real-world data. Therefore, while PCA is a valuable tool, it should be applied judiciously and in conjunction with domain knowledge to ensure that it aligns with the goals of our analysis or modeling task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q.No-06    You are working on a project to build a model to predict stock prices. The dataset contains many features, such as company financial data and market trends. Explain how you would use PCA to reduce the dimensionality of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans :-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`Principal Component Analysis` (PCA)** is a dimensionality reduction technique commonly used in data analysis and machine learning to reduce the number of features or dimensions in a dataset while preserving as much relevant information as possible. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Data Preprocessing :-**\n",
    "\n",
    "   - Start by collecting and preprocessing your dataset, which should include various features related to company financial data and market trends over time.\n",
    "\n",
    "   - Standardize or normalize the data: PCA is sensitive to the scale of the variables, so it's important to standardize or normalize the features to have a mean of 0 and a standard deviation of 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Calculate Covariance Matrix :-**\n",
    "\n",
    "   - Compute the covariance matrix of your standardized dataset. The covariance matrix represents the relationships between different features and is crucial for PCA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Eigendecomposition :-**\n",
    "\n",
    "   - Perform eigendecomposition (or singular value decomposition) on the covariance matrix to obtain the eigenvalues and eigenvectors.\n",
    "\n",
    "   - Eigenvalues represent the variance explained by each principal component, while eigenvectors represent the directions in which the data varies the most."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. **Select Principal Components :-**\n",
    "\n",
    "   - Sort the eigenvalues in descending order to determine the most significant principal components. You can visualize the explained variance by plotting the cumulative explained variance versus the number of components. Typically, you'll aim to retain a significant portion of the variance, often around 95% or more.\n",
    "\n",
    "   - Choose the top N eigenvectors (principal components) corresponding to the highest eigenvalues to retain in your reduced dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. **Project Data onto Principal Components :-**\n",
    "\n",
    "   - Project your original data onto the selected principal components. This involves multiplying your data by the matrix of selected eigenvectors. The result is a new dataset with reduced dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. **Data Reconstruction (Optional) :-**\n",
    "\n",
    "   - If needed, you can reverse the dimensionality reduction process by multiplying the reduced dataset by the transposed matrix of selected eigenvectors. This will yield a dataset that approximates the original data but with reduced dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. **Model Building :-**\n",
    "\n",
    "   - Train your stock price prediction model on the reduced dataset, which now has fewer features. You can use various machine learning algorithms like linear regression, support vector machines, or neural networks, depending on the complexity of your problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`Benefits of using PCA for dimensionality reduction in a stock price prediction project`:**\n",
    "- Reduced computational complexity: Fewer features mean faster training and inference times.\n",
    "\n",
    "- Mitigation of multicollinearity: PCA can help address issues of multicollinearity in your dataset.\n",
    "\n",
    "- Improved model generalization: By removing noise and redundancy, PCA can lead to more robust and generalized models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`However`, it's essential to strike a balance between dimensionality reduction and model performance. While PCA can be powerful in reducing dimensionality, you should carefully assess the impact on predictive accuracy and choose an appropriate number of principal components to retain. Experiment with different numbers of components and monitor the model's performance to find the optimal balance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q.No-07    For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the values to a range of -1 to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans :-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given Values :- [1, 5, 10, 15, 20]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Original Values</th>\n",
       "      <th>Scaled Values</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>-0.578947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>-0.052632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15</td>\n",
       "      <td>0.473684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Original Values  Scaled Values\n",
       "0                1      -1.000000\n",
       "1                5      -0.578947\n",
       "2               10      -0.052632\n",
       "3               15       0.473684\n",
       "4               20       1.000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Create a DataFrame with your data\n",
    "Given_Values = [1, 5, 10, 15, 20]\n",
    "print(f\"Given Values :- {Given_Values}\\n\")\n",
    "Q_No_07_df = pd.DataFrame(Given_Values, columns=['Original Values'])\n",
    "\n",
    "# Create a MinMaxScaler instance\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "\n",
    "# Fit and transform the data\n",
    "scaled_values = scaler.fit_transform(Q_No_07_df[['Original Values']])\n",
    "\n",
    "# Create a new DataFrame to store the scaled values\n",
    "Q_No_07_df['Scaled Values'] = pd.DataFrame(scaled_values)\n",
    "\n",
    "# Display the scaled DataFrame\n",
    "display(Q_No_07_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q.No-08    For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform Feature Extraction using PCA. How many principal components would you choose to retain, and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans :-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of principal components to retain in **`PCA` (Principal Component Analysis)** depends on the specific goals of your analysis and the trade-off between reducing dimensionality and retaining information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`These are the steps to decide how many principal components to keep` :-**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Standardize the Data**: Before performing PCA, it's essential to standardize the data (subtract the mean and divide by the standard deviation) since PCA is sensitive to the scale of the features. This ensures that all features contribute equally to the analysis.\n",
    "\n",
    "2. **Calculate Covariance Matrix**: Calculate the covariance matrix of the standardized data. The covariance matrix represents the relationships between the features.\n",
    "\n",
    "3. **Eigenvalue Decomposition**: Perform eigenvalue decomposition on the covariance matrix. This will give you a set of eigenvalues and eigenvectors.\n",
    "\n",
    "4. **Eigenvalues**: Sort the eigenvalues in descending order. These eigenvalues represent the variance explained by each principal component. The eigenvalues indicate how much of the total variance in the data each principal component explains.\n",
    "\n",
    "5. **Explained Variance**: Calculate the cumulative explained variance by summing the eigenvalues from highest to lowest. This will help you understand how much variance is retained as you include more principal components.\n",
    "\n",
    "6. **Set a Threshold**: Decide on a threshold for the amount of variance you want to retain. A common threshold is to retain a certain percentage of the total variance, such as 95% or 99%.\n",
    "\n",
    "7. **Select Principal Components**: Choose the number of principal components that collectively explain the desired amount of variance based on the threshold you set.\n",
    "\n",
    "8. **Visualize and Validate**: Plot the explained variance versus the number of principal components to visually inspect how quickly the explained variance saturates. This can help you decide on the number of components to retain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The number of principal components** we choose to retain should strike a balance between reducing dimensionality (keeping it as low as possible) and retaining sufficient information. Retaining too few principal components may result in a loss of important information, while retaining too many may not provide significant benefits and can lead to overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Typically`, we would choose the number of components that collectively explain a high percentage of the total variance, such as 95% or 99%. The specific threshold you select depends on your application and the trade-off we are willing to make between dimensionality reduction and information preservation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's important to note that there is no one-size-fits-all answer to how many principal components to retain; it depends on the characteristics of your dataset and your analysis goals. You can experiment with different thresholds and evaluate the impact on the performance of your downstream tasks (e.g., classification or regression) to make an informed decision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                        END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
